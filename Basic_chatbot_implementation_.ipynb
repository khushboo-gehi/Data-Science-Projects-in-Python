{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushboo-gehi/Py-ML-DL/blob/main/Basic_chatbot_implementation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zj9h92wUDPN"
      },
      "source": [
        "**Importing the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4BmyocFbb0MJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85QE5FDSUKqU"
      },
      "source": [
        "**Importing and reading the corpus**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jouIkYEkb9Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39552267-7b03-4cd0-9acd-a01384de1ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "f=open('chatbot.txt','r',errors = 'ignore')\n",
        "raw_doc=f.read()\n",
        "raw_doc=raw_doc.lower() #Converts text to lowercase\n",
        "nltk.download('punkt') #Using the Punkt tokenizer\n",
        "nltk.download('wordnet') #Using the WordNet dictionary\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc) #Converts doc to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw_doc) #Converts doc to list of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXgGkVeUSUb"
      },
      "source": [
        "**Example of sentance tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Swu4WRVncPR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f203bd2f-f007-401d-e0d0-4e7f289085d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['using lstm to navigate robots\\n\\nlong short-term memory networks or lstms are a type of recurrent neural networks that reuse the output of a previous step as an input for the next.',\n",
              " 'they are useful making predictions based on sequences.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "sent_tokens[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtkzd0KhUWJT"
      },
      "source": [
        "**Example of word tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hcwrvmWicaLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461df269-8526-4a04-e105-f3588b710f9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['using', 'lstm']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "word_tokens[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvvYcZZ9UbVD"
      },
      "source": [
        "**Text preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YbZllVqBcc78"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLX8WBE4UgOr"
      },
      "source": [
        "**Defining the greeting function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dLOqphibchJM"
      },
      "outputs": [],
      "source": [
        "GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"namaste\")\n",
        "GREET_RESPONSES = [\"hi\", \"hey\", \"*smiley*\", \"hi there\", \"hello\", \"welcome!\"]\n",
        "def greet(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREET_INPUTS:\n",
        "            return random.choice(GREET_RESPONSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJhFmyRCUm4j"
      },
      "source": [
        "**Response generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "eo7Kv52HcjG0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "JEHZesw3cnNM"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response=robo1_response+\"Sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response+sent_tokens[idx]\n",
        "    return robo1_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q-iY_o1Utas"
      },
      "source": [
        "**Defining conversation start/end protocols**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wxzENVDgdNGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e167ee31-a1f1-4b9d-8f4f-5381807bc828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: Hi! Let's chat about AI and Data Science Usecases! In case you want to exit, type Bye!\n",
            "namaste\n",
            "BOT: hello\n",
            "tell me something about face detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: typically a face detection system captures an image, detects face, normalizes it, extracts & matches the feature vectors to identify a face.\n",
            "what is regression?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: staff absence analysis using logistic regression\n",
            "\n",
            "logistic regression is a supervised learning technique that predicts discrete or binary values using a set of independent variables.\n",
            "is that a joke?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: Sorry! I don't understand you\n",
            "bye!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: Sorry! I don't understand you\n",
            "i said bye\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT: clustered around the selected topic are topics 5,4,3 and 2. the bigger the circles, the higher the frequency of the said terms and the further the distance between, the lesser the similarity between terms amongst topics.\n",
            "bye\n",
            "BOT: Bye!\n"
          ]
        }
      ],
      "source": [
        "flag=True\n",
        "print(\"BOT: Hi! Let's chat about AI and Data Science Usecases! In case you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"BOT: You are welcome..\")\n",
        "        else:\n",
        "            if(greet(user_response)!=None):\n",
        "                print(\"BOT: \"+greet(user_response))\n",
        "            else:\n",
        "                sent_tokens.append(user_response)\n",
        "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "                final_words=list(set(word_tokens))\n",
        "                print(\"BOT: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"BOT: Bye!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5yXpRQMXopU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Basic_chatbot_implementation .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}